from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lower, trim
from awsglue.context import GlueContext

# Initialize
spark = SparkSession.builder.appName("SKUMasterETL").getOrCreate()
glueContext = GlueContext(spark.sparkContext)

# Load sku_master from Glue Catalog
sku_master = glueContext.create_dynamic_frame.from_catalog(
    database="pizza_raw_rds-aahash",
    table_name="pizzadb_aahash_sku_master"
).toDF()

# 1. Remove rows with null sku_id
sku_master_clean = sku_master.filter(col("sku_id").isNotNull())

# 2. Trim and lowercase item_name and category
sku_master_cleaned = sku_master_clean \
    .withColumn("item_name", lower(trim(col("item_name")))) \
    .withColumn("category", lower(trim(col("category"))))

# 3. Write cleaned sku_master to S3 as Parquet
sku_master_cleaned.write.mode("overwrite").parquet("s3://aahash-project3/pizza/cleaned_sku_master/")

print("âœ… Cleaned sku_master saved to S3.")
